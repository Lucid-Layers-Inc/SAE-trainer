{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5111c6c3-1199-4c5a-bdd7-2e9991cb0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "from typing import Any, Dict, Tuple\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "from safetensors.torch import load_file, save_file\n",
    "import os\n",
    "\n",
    "from sae_lens import HookedSAETransformer, StandardSAE, SAEConfig\n",
    "from safetensors.torch import load_file as safetensors_load_file\n",
    "\n",
    "\n",
    "def _dtype_to_cfg_str(dtype: torch.dtype | str | None) -> str:\n",
    "    if dtype is None:\n",
    "        return \"torch.float32\"\n",
    "    if isinstance(dtype, torch.dtype):\n",
    "        return str(dtype)\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def _device_to_cfg_str(device: torch.device | str | None) -> str:\n",
    "    if device is None:\n",
    "        return \"cpu\"\n",
    "    if isinstance(device, torch.device):\n",
    "        return str(device)\n",
    "    return device\n",
    "\n",
    "\n",
    "def _build_sae_cfg_from_training(\n",
    "    *,\n",
    "    model_name: str,\n",
    "    hook_layer: int,\n",
    "    d_in: int,\n",
    "    d_sae: int,\n",
    "    context_size: int = 128,\n",
    "    dataset_path: str = \"ashaba1in/small_openwebtext\",\n",
    "    hook_name: str = \"blocks.{layer}.hook_{target}\",  # hook_mlp_in  hook_resid_pre,\n",
    "    target: str = \"resid_pre\",  # mid_pre, resid_pre,\n",
    "    device: torch.device | str | None = \"cuda\",\n",
    "    dtype: torch.dtype | str | None = \"torch.float32\",\n",
    "    hook_head_index: Optional[int] = None,\n",
    ") -> SAEConfig:\n",
    "    target = \"mlp_in\" if target == \"mid_pre\" else \"resid_pre\"\n",
    "    \n",
    "    cfg_dict = {\n",
    "        \"architecture\": \"standard\",\n",
    "        \"d_in\": int(d_in),\n",
    "        \"d_sae\": int(d_sae),\n",
    "        \"activation_fn_str\": \"relu\",\n",
    "        \"activation_fn_kwargs\": {},\n",
    "        \"apply_b_dec_to_input\": True,\n",
    "        \"finetuning_scaling_factor\": False,\n",
    "        \"context_size\": int(context_size),\n",
    "        \"model_name\": model_name,\n",
    "        \"hook_name\": hook_name.format(layer=hook_layer, target=target),\n",
    "        \"hook_layer\": int(hook_layer),\n",
    "        \"hook_head_index\": hook_head_index,\n",
    "        \"prepend_bos\": False,\n",
    "        \"dataset_path\": dataset_path,\n",
    "        \"dataset_trust_remote_code\": False,\n",
    "        \"normalize_activations\": \"none\",\n",
    "        \"dtype\": _dtype_to_cfg_str(dtype),\n",
    "        \"device\": _device_to_cfg_str(device),\n",
    "        \"sae_lens_training_version\": None,\n",
    "        \"neuronpedia_id\": None,\n",
    "        \"model_from_pretrained_kwargs\": {},\n",
    "        \"seqpos_slice\": (None,),\n",
    "    }\n",
    "    return SAEConfig.from_dict(cfg_dict)\n",
    "\n",
    "\n",
    "def get_custom_hf_model(model_name: str, kwargs: Dict[str, Any] = {}) -> HookedTransformer:\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        **kwargs\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "    )\n",
    "    \n",
    "    hf_config = hf_model.config\n",
    "    \n",
    "    # Создаем конфигурацию для TransformerLens\n",
    "    # Ограничиваем размер контекста для экономии памяти\n",
    "    max_ctx = min(hf_config.max_position_embeddings, 2048)\n",
    "    \n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=hf_config.num_hidden_layers,\n",
    "        d_model=hf_config.hidden_size,\n",
    "        d_head=hf_config.hidden_size // hf_config.num_attention_heads,\n",
    "        n_heads=hf_config.num_attention_heads,\n",
    "        d_mlp=hf_config.intermediate_size,\n",
    "        d_vocab=hf_config.vocab_size,\n",
    "        n_ctx=max_ctx,  # Ограничиваем размер контекста\n",
    "        act_fn=hf_config.hidden_act,  # Llama использует SiLU\n",
    "        model_name=model_name,\n",
    "        normalization_type=\"RMS\",  # Llama использует RMSNorm\n",
    "        device=\"cpu\", \n",
    "        use_hook_mlp_in=True,\n",
    "    )\n",
    "    \n",
    "    model = HookedTransformer(cfg)\n",
    "    \n",
    "    model.load_state_dict(hf_model.state_dict(), strict=False)\n",
    "    model.set_tokenizer(tokenizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_hookedtrans_and_sae(\n",
    "    hf_model = \"ExplosionNuclear/Llama-2.3-3B-Instruct-special\", \n",
    "    layer = 1, \n",
    "    hook_target = \"mid_pre\"  # mid_pre  resid_pre\n",
    ") -> Tuple[HookedTransformer, StandardSAE]:\n",
    "\n",
    "    hook_trans = get_custom_hf_model(hf_model)\n",
    "    hook_trans.to(\"cuda\")\n",
    "    \n",
    "    repo_id = \"Lucid-Layers-Inc/Llama-2.3-3B-Instruct-special-hook_mlp_in-SAE\"\n",
    "    #repo_id = repo_id.replace(\"special\", \"special-merged\") if \"special-merged\" in hf_model else repo_id\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=f\"ExplosionNuclear-Llama-2.3-3B-Instruct-special_layer-{layer}.hook_mlp_in_30720.safetensors\",\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    \n",
    "    weights = load_file(local_path)\n",
    "    hidden_dim, d_in = weights[\"W_dec\"].shape\n",
    "    \n",
    "    sae_cfg = _build_sae_cfg_from_training(d_sae=hidden_dim, d_in=d_in, hook_layer=layer, model_name=hf_model, target=hook_target)\n",
    "    sae = StandardSAE(sae_cfg)\n",
    "    sae.load_state_dict(weights, strict=True)\n",
    "    return hook_trans, sae\n",
    "\n",
    "def load_sae_only(\n",
    "    hf_model = \"ExplosionNuclear/Llama-2.3-3B-Instruct-special\", \n",
    "    layer = 1, \n",
    "    hook_target = \"mid_pre\"  # mid_pre  resid_pre\n",
    ") -> StandardSAE:\n",
    "    \"\"\"Загружает только SAE без модели для проверки конфигурации\"\"\"\n",
    "    \n",
    "    repo_id = \"Lucid-Layers-Inc/Llama-2.3-3B-Instruct-special-hook_resid_pre-SAE\"\n",
    "    repo_id = repo_id.replace(\"special\", \"special-merged\") if \"special-merged\" in hf_model else repo_id\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=f\"ExplosionNuclear-Llama-2.3-3B-Instruct-special_layer-{layer}.hook_mlp_in_30720.safetensors\",\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    \n",
    "    weights = load_file(local_path)\n",
    "    hidden_dim, d_in = weights[\"W_dec\"].shape\n",
    "    \n",
    "    sae_cfg = _build_sae_cfg_from_training(d_sae=hidden_dim, d_in=d_in, hook_layer=layer, model_name=hf_model, target=hook_target)\n",
    "    sae = StandardSAE(sae_cfg)\n",
    "    sae.load_state_dict(weights, strict=True)\n",
    "    return sae\n",
    "\n",
    "def get_hook_name(layer: int, hook_target: str) -> str:\n",
    "    \"\"\"Получает hook_name для указанного слоя и цели\"\"\"\n",
    "    target = \"mlp_in\" if hook_target == \"mid_pre\" else \"resid_pre\"\n",
    "    return f\"blocks.{layer}.hook_{target}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad36d13-85de-48bb-83ac-4cd3e05c6007",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sae-lens\n",
      "  Downloading sae_lens-6.11.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: transformer-lens in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (1.19.0)\n",
      "Collecting sae-dashboard\n",
      "  Downloading sae_dashboard-0.7.2-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: babe<0.0.8,>=0.0.7 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (0.0.7)\n",
      "Collecting datasets>=3.1.0 (from sae-lens)\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (3.9.1)\n",
      "Requirement already satisfied: plotly>=5.19.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (5.24.1)\n",
      "Requirement already satisfied: plotly-express>=0.4.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (1.1.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (6.0.2)\n",
      "Requirement already satisfied: safetensors<1.0.0,>=0.4.2 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (0.6.2)\n",
      "Collecting simple-parsing<0.2.0,>=0.1.6 (from sae-lens)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tenacity>=9.0.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (9.1.2)\n",
      "Collecting transformer-lens\n",
      "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (4.56.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-lens) (4.15.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (1.10.1)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.0.3)\n",
      "Requirement already satisfied: einops>=0.6.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.8.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.3.2)\n",
      "Requirement already satisfied: numpy<2,>=1.24 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (2.2.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (13.9.4)\n",
      "Requirement already satisfied: sentencepiece in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.2.1)\n",
      "Requirement already satisfied: torch>=2.6 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (4.67.1)\n",
      "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer-lens)\n",
      "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typeguard<5.0,>=4.2 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (4.4.4)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformer-lens) (0.21.3)\n",
      "Requirement already satisfied: py2store in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.21)\n",
      "Requirement already satisfied: graze in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.29)\n",
      "Requirement already satisfied: click in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (2025.9.1)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing<0.2.0,>=0.1.6->sae-lens)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: filelock in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (25.0)\n",
      "Requirement already satisfied: requests in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (0.22.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5.0.0,>=4.38.1->sae-lens) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5.0.0,>=4.38.1->sae-lens) (1.1.9)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.4 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-dashboard) (0.6.7)\n",
      "Collecting datasets>=3.1.0 (from sae-lens)\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting decode-clt<0.0.2,>=0.0.1 (from sae-dashboard)\n",
      "  Downloading decode_clt-0.0.1-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: eindex-callum<0.2.0,>=0.1.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-dashboard) (0.1.1)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer-lens)\n",
      "  Downloading jaxtyping-0.2.38-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.4 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sae-dashboard) (3.10.6)\n",
      "Collecting safetensors<1.0.0,>=0.4.2 (from sae-lens)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting typer<0.13.0,>=0.12.3 (from sae-dashboard)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.4->sae-dashboard) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.4->sae-dashboard) (0.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from datasets>=3.1.0->sae-lens) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from datasets>=3.1.0->sae-lens) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from datasets>=3.1.0->sae-lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from datasets>=3.1.0->sae-lens) (0.70.16)\n",
      "Collecting torchvision>=0.15.1 (from decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio>=2.0.1 (from decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached torchaudio-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting nnsight>=0.1.0 (from decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Downloading nnsight-0.5.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: zstandard in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.24.0)\n",
      "Collecting h5py (from decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached h5py-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (3.12.15)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from jaxtyping>=0.2.11->transformer-lens) (0.1.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.8.4->sae-dashboard) (2.9.0.post0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from rich>=12.6.0->transformer-lens) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from rich>=12.6.0->transformer-lens) (2.19.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from torch>=2.6->transformer-lens) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from triton==3.4.0->torch>=2.6->transformer-lens) (80.9.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from typer<0.13.0,>=0.12.3->sae-dashboard) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.4->sae-dashboard) (1.1.0)\n",
      "Requirement already satisfied: psutil in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from accelerate>=0.23.0->transformer-lens) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.1.0->sae-lens) (3.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
      "Collecting astor (from nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cloudpickle (from nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting python-socketio[client] (from nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached python_socketio-5.13.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (2.9.2)\n",
      "Collecting toml (from nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipython in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (9.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
      "Collecting statsmodels>=0.9.0 (from plotly-express>=0.4.1->sae-lens)\n",
      "  Using cached statsmodels-0.14.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: scipy>=0.18 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from plotly-express>=0.4.1->sae-lens) (1.16.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from plotly-express>=0.4.1->sae-lens) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from pydantic>=2.9.0->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from pydantic>=2.9.0->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.8.4->sae-dashboard) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.38.1->sae-lens) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.38.1->sae-lens) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.38.1->sae-lens) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.6->transformer-lens) (1.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens) (3.20.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from wandb>=0.13.5->transformer-lens) (2.35.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (5.0.2)\n",
      "Requirement already satisfied: dol in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from graze->babe<0.0.8,>=0.0.7->sae-lens) (0.3.19)\n",
      "Requirement already satisfied: decorator in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (3.0.52)\n",
      "Requirement already satisfied: stack_data in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from jedi>=0.16->ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from pexpect>4.3->ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from jinja2->torch>=2.6->transformer-lens) (3.0.2)\n",
      "Requirement already satisfied: config2py in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.38)\n",
      "Requirement already satisfied: importlib_resources in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (6.5.2)\n",
      "Requirement already satisfied: i2 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.48)\n",
      "Collecting bidict>=0.21.0 (from python-socketio[client]->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.11.0 (from python-socketio[client]->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached python_engineio-4.12.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from python-socketio[client]->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (1.8.0)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.11.0->python-socketio[client]->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from stack_data->ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from stack_data->ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from stack_data->ipython->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.2.3)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight>=0.1.0->decode-clt<0.0.2,>=0.0.1->sae-dashboard) (0.16.0)\n",
      "Downloading sae_lens-6.11.0-py3-none-any.whl (147 kB)\n",
      "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
      "Downloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading sae_dashboard-0.7.2-py3-none-any.whl (125 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading decode_clt-0.0.1-py3-none-any.whl (151 kB)\n",
      "Downloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
      "Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading nnsight-0.5.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (92 kB)\n",
      "Using cached statsmodels-0.14.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.5 MB)\n",
      "Using cached torchaudio-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "Using cached torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached h5py-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Using cached python_socketio-5.13.0-py3-none-any.whl (77 kB)\n",
      "Using cached bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Using cached python_engineio-4.12.2-py3-none-any.whl (59 kB)\n",
      "Using cached simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: transformers-stream-generator\n",
      "\u001b[33m  DEPRECATION: Building 'transformers-stream-generator' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'transformers-stream-generator'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12525 sha256=457491a7f8b0f2f41232ef8a63365ba5841e4d0dd4aa21dc2ffe91cee22336e7\n",
      "  Stored in directory: /workspace-SR008.fs2/.cache/pip/wheels/23/e8/f0/b3c58c12d1ffe60bcc8c7d121115f26b2c1878653edfca48db\n",
      "Successfully built transformers-stream-generator\n",
      "Installing collected packages: wsproto, toml, safetensors, jaxtyping, h5py, docstring-parser, cloudpickle, bidict, astor, simple-websocket, simple-parsing, typer, statsmodels, python-engineio, torchvision, torchaudio, python-socketio, datasets, transformers-stream-generator, transformer-lens, nnsight, sae-lens, decode-clt, sae-dashboard\n",
      "\u001b[2K  Attempting uninstall: safetensors\n",
      "\u001b[2K    Found existing installation: safetensors 0.6.2\n",
      "\u001b[2K    Uninstalling safetensors-0.6.2:237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [safetensors]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.6.237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [safetensors]\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [safetensors]\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages/~afetensors'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[2K  Attempting uninstall: jaxtyping\n",
      "\u001b[2K    Found existing installation: jaxtyping 0.3.237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [safetensors]\n",
      "\u001b[2K    Uninstalling jaxtyping-0.3.2:7m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [safetensors]\n",
      "\u001b[2K      Successfully uninstalled jaxtyping-0.3.2;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [safetensors]\n",
      "\u001b[2K  Attempting uninstall: typer━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/24\u001b[0m [simple-parsing]\n",
      "\u001b[2K    Found existing installation: typer 0.9.0249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/24\u001b[0m [simple-parsing]\n",
      "\u001b[2K    Uninstalling typer-0.9.0:━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/24\u001b[0m [simple-parsing]\n",
      "\u001b[2K      Successfully uninstalled typer-0.9.02;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/24\u001b[0m [simple-parsing]\n",
      "\u001b[2K  Attempting uninstall: datasets━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m16/24\u001b[0m [python-socketio]\n",
      "\u001b[2K    Found existing installation: datasets 2.21.0━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m17/24\u001b[0m [datasets]-socketio]\n",
      "\u001b[2K    Uninstalling datasets-2.21.0:━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m17/24\u001b[0m [datasets]\n",
      "\u001b[2K      Successfully uninstalled datasets-2.21.0━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m17/24\u001b[0m [datasets]\n",
      "\u001b[2K  Attempting uninstall: transformer-lens━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m17/24\u001b[0m [datasets]\n",
      "\u001b[2K    Found existing installation: transformer-lens 1.19.0\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m19/24\u001b[0m [transformer-lens]\n",
      "\u001b[2K    Uninstalling transformer-lens-1.19.0:━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m19/24\u001b[0m [transformer-lens]\n",
      "\u001b[2K      Successfully uninstalled transformer-lens-1.19.038;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m19/24\u001b[0m [transformer-lens]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [sae-dashboard]\u001b[32m23/24\u001b[0m [sae-dashboard]clt]-lens]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlspace 0.23.4 requires pandas==1.5.0, but you have pandas 2.2.2 which is incompatible.\n",
      "mlspace 0.23.4 requires typer[all]<0.10.0,>=0.9.0, but you have typer 0.12.5 which is incompatible.\n",
      "mlspace 0.23.4 requires urllib3<2.0.0,>=1.26.18, but you have urllib3 2.5.0 which is incompatible.\n",
      "client-lib 0.3.9 requires typer==0.9, but you have typer 0.12.5 which is incompatible.\n",
      "client-lib 0.3.9 requires typing-extensions==4.10.0, but you have typing-extensions 4.15.0 which is incompatible.\n",
      "client-lib 0.3.9 requires urllib3==1.26.18, but you have urllib3 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astor-0.8.1 bidict-0.23.1 cloudpickle-3.1.1 datasets-3.6.0 decode-clt-0.0.1 docstring-parser-0.17.0 h5py-3.14.0 jaxtyping-0.2.38 nnsight-0.5.2 python-engineio-4.12.2 python-socketio-5.13.0 sae-dashboard-0.7.2 sae-lens-6.11.0 safetensors-0.4.5 simple-parsing-0.1.7 simple-websocket-1.1.0 statsmodels-0.14.5 toml-0.10.2 torchaudio-2.8.0 torchvision-0.23.0 transformer-lens-2.16.1 transformers-stream-generator-0.0.5 typer-0.12.5 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sae-lens transformer-lens sae-dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36795387-5871-4600-80fb-f27474145375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650aa6cbbbb341b9aafebc639ed1d9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/micromamba/envs/ilya/lib/python3.11/site-packages/sae_lens/saes/sae.py:249: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, sae = load_hookedtrans_and_sae(layer=10, hook_target=\"resid_pre\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228e631d-b675-4759-b46c-2b62b6ce3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache keys: ['blocks.10.hook_resid_pre', 'blocks.10.hook_mlp_in']\n",
      "tensor(150.1659, device='cuda:0')\n",
      "torch.Size([21, 3072])\n",
      "norm tensor([189.4258, 186.5565, 184.8637, 183.1293, 183.7237, 181.9275, 182.0390,\n",
      "        180.5799, 179.9156, 180.4675, 179.8455, 179.7326, 179.7034, 180.7181,\n",
      "        180.1364, 179.9122, 179.6683, 178.5136, 178.7495, 177.5304, 177.3266],\n",
      "       device='cuda:0')\n",
      "tensor([[14366,  1447, 20511, 23265, 28850, 11604,  3740,  4443, 14864,  4111],\n",
      "        [25326, 11559, 15454, 12909,  4624, 29818, 16946, 10761, 24041, 16843],\n",
      "        [26037, 15454,     0,     1,     6,     7,     5,     4,     2,     3],\n",
      "        [26037,     1,     3,     6,     7,     0,     2,     5,     8,     4],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [16314,     1,     3,     6,     7,     0,     2,     5,     8,     4],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [    7,     6,     4,     5,     1,     0,     2,     3,     8,     9],\n",
      "        [16314, 11914,     0,     1,     6,     7,     5,     4,     2,     3],\n",
      "        [16314,     1,     3,     6,     7,     0,     2,     5,     8,     4],\n",
      "        [16314, 19824,     0,     1,     6,     7,     5,     4,     2,     3],\n",
      "        [16314, 11914, 17238,     0,     6,     3,     5,     4,     1,     2],\n",
      "        [16314, 19824, 30706,     0,     6,     3,     5,     4,     1,     2],\n",
      "        [16314, 11914, 19824, 15377, 14864,  1261, 14386, 20933, 30706,     0],\n",
      "        [16314,  1261, 11914, 29818, 15377,  2814, 27201,     2,     1,     0],\n",
      "        [16314, 29818, 11914, 10494,   886, 19824, 13311, 14864, 26270, 26845],\n",
      "        [16314, 11914, 19824,     0,     6,     3,     5,     4,     1,     2]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"Max, Mark and Smith were in the empty dark room. Smith left. Mark gave a drink to\"\n",
    "\n",
    "tokens = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(prompt)\n",
    "\n",
    "layer = 10\n",
    "hook_name = f\"blocks.{layer}.hook_mlp_in\"\n",
    "hook_name1 = f\"blocks.{layer}.hook_resid_pre\"\n",
    "\n",
    "_, cache = model.run_with_cache(\n",
    "    tokens,\n",
    "    names_filter=[hook_name, hook_name1],\n",
    "    remove_batch_dim=True\n",
    ")\n",
    "\n",
    "print(f\"Cache keys: {list(cache.keys())}\")\n",
    "resid_pre1 = cache[hook_name1]\n",
    "resid_pre = cache[hook_name] \n",
    "\n",
    "_, sae_cache_pre = sae.run_with_cache(resid_pre1)\n",
    "sae_out  = sae_cache_pre[\"hook_sae_output\"]\n",
    "\n",
    "errors = resid_pre1 - sae_out\n",
    "print(torch.norm(resid_pre1[1,:]))\n",
    "print(errors.shape)\n",
    "print(\"norm\", torch.norm(errors, dim = -1))\n",
    "\n",
    "\n",
    "acts_post = sae_cache_pre[\"hook_sae_acts_post\"] # [n_tokens, d_sae]\n",
    "top_vals, top_idx = torch.topk(acts_post, k=10, dim=-1)\n",
    "\n",
    "print(top_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b250cb7-25f1-4736-8361-0c2835479353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.hook_mlp_in', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.hook_mlp_in', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.hook_mlp_in', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.hook_mlp_in', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.hook_mlp_in', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.hook_mlp_in', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.hook_mlp_in', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.hook_mlp_in', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.hook_mlp_in', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.hook_mlp_in', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.hook_mlp_in', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.hook_mlp_in', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_pre', 'blocks.12.ln1.hook_scale', 'blocks.12.ln1.hook_normalized', 'blocks.12.attn.hook_q', 'blocks.12.attn.hook_k', 'blocks.12.attn.hook_v', 'blocks.12.attn.hook_attn_scores', 'blocks.12.attn.hook_pattern', 'blocks.12.attn.hook_z', 'blocks.12.hook_attn_out', 'blocks.12.hook_resid_mid', 'blocks.12.hook_mlp_in', 'blocks.12.ln2.hook_scale', 'blocks.12.ln2.hook_normalized', 'blocks.12.mlp.hook_pre', 'blocks.12.mlp.hook_post', 'blocks.12.hook_mlp_out', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_pre', 'blocks.13.ln1.hook_scale', 'blocks.13.ln1.hook_normalized', 'blocks.13.attn.hook_q', 'blocks.13.attn.hook_k', 'blocks.13.attn.hook_v', 'blocks.13.attn.hook_attn_scores', 'blocks.13.attn.hook_pattern', 'blocks.13.attn.hook_z', 'blocks.13.hook_attn_out', 'blocks.13.hook_resid_mid', 'blocks.13.hook_mlp_in', 'blocks.13.ln2.hook_scale', 'blocks.13.ln2.hook_normalized', 'blocks.13.mlp.hook_pre', 'blocks.13.mlp.hook_post', 'blocks.13.hook_mlp_out', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_pre', 'blocks.14.ln1.hook_scale', 'blocks.14.ln1.hook_normalized', 'blocks.14.attn.hook_q', 'blocks.14.attn.hook_k', 'blocks.14.attn.hook_v', 'blocks.14.attn.hook_attn_scores', 'blocks.14.attn.hook_pattern', 'blocks.14.attn.hook_z', 'blocks.14.hook_attn_out', 'blocks.14.hook_resid_mid', 'blocks.14.hook_mlp_in', 'blocks.14.ln2.hook_scale', 'blocks.14.ln2.hook_normalized', 'blocks.14.mlp.hook_pre', 'blocks.14.mlp.hook_post', 'blocks.14.hook_mlp_out', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_pre', 'blocks.15.ln1.hook_scale', 'blocks.15.ln1.hook_normalized', 'blocks.15.attn.hook_q', 'blocks.15.attn.hook_k', 'blocks.15.attn.hook_v', 'blocks.15.attn.hook_attn_scores', 'blocks.15.attn.hook_pattern', 'blocks.15.attn.hook_z', 'blocks.15.hook_attn_out', 'blocks.15.hook_resid_mid', 'blocks.15.hook_mlp_in', 'blocks.15.ln2.hook_scale', 'blocks.15.ln2.hook_normalized', 'blocks.15.mlp.hook_pre', 'blocks.15.mlp.hook_post', 'blocks.15.hook_mlp_out', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_pre', 'blocks.16.ln1.hook_scale', 'blocks.16.ln1.hook_normalized', 'blocks.16.attn.hook_q', 'blocks.16.attn.hook_k', 'blocks.16.attn.hook_v', 'blocks.16.attn.hook_attn_scores', 'blocks.16.attn.hook_pattern', 'blocks.16.attn.hook_z', 'blocks.16.hook_attn_out', 'blocks.16.hook_resid_mid', 'blocks.16.hook_mlp_in', 'blocks.16.ln2.hook_scale', 'blocks.16.ln2.hook_normalized', 'blocks.16.mlp.hook_pre', 'blocks.16.mlp.hook_post', 'blocks.16.hook_mlp_out', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_pre', 'blocks.17.ln1.hook_scale', 'blocks.17.ln1.hook_normalized', 'blocks.17.attn.hook_q', 'blocks.17.attn.hook_k', 'blocks.17.attn.hook_v', 'blocks.17.attn.hook_attn_scores', 'blocks.17.attn.hook_pattern', 'blocks.17.attn.hook_z', 'blocks.17.hook_attn_out', 'blocks.17.hook_resid_mid', 'blocks.17.hook_mlp_in', 'blocks.17.ln2.hook_scale', 'blocks.17.ln2.hook_normalized', 'blocks.17.mlp.hook_pre', 'blocks.17.mlp.hook_post', 'blocks.17.hook_mlp_out', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_pre', 'blocks.18.ln1.hook_scale', 'blocks.18.ln1.hook_normalized', 'blocks.18.attn.hook_q', 'blocks.18.attn.hook_k', 'blocks.18.attn.hook_v', 'blocks.18.attn.hook_attn_scores', 'blocks.18.attn.hook_pattern', 'blocks.18.attn.hook_z', 'blocks.18.hook_attn_out', 'blocks.18.hook_resid_mid', 'blocks.18.hook_mlp_in', 'blocks.18.ln2.hook_scale', 'blocks.18.ln2.hook_normalized', 'blocks.18.mlp.hook_pre', 'blocks.18.mlp.hook_post', 'blocks.18.hook_mlp_out', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_pre', 'blocks.19.ln1.hook_scale', 'blocks.19.ln1.hook_normalized', 'blocks.19.attn.hook_q', 'blocks.19.attn.hook_k', 'blocks.19.attn.hook_v', 'blocks.19.attn.hook_attn_scores', 'blocks.19.attn.hook_pattern', 'blocks.19.attn.hook_z', 'blocks.19.hook_attn_out', 'blocks.19.hook_resid_mid', 'blocks.19.hook_mlp_in', 'blocks.19.ln2.hook_scale', 'blocks.19.ln2.hook_normalized', 'blocks.19.mlp.hook_pre', 'blocks.19.mlp.hook_post', 'blocks.19.hook_mlp_out', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_pre', 'blocks.20.ln1.hook_scale', 'blocks.20.ln1.hook_normalized', 'blocks.20.attn.hook_q', 'blocks.20.attn.hook_k', 'blocks.20.attn.hook_v', 'blocks.20.attn.hook_attn_scores', 'blocks.20.attn.hook_pattern', 'blocks.20.attn.hook_z', 'blocks.20.hook_attn_out', 'blocks.20.hook_resid_mid', 'blocks.20.hook_mlp_in', 'blocks.20.ln2.hook_scale', 'blocks.20.ln2.hook_normalized', 'blocks.20.mlp.hook_pre', 'blocks.20.mlp.hook_post', 'blocks.20.hook_mlp_out', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_pre', 'blocks.21.ln1.hook_scale', 'blocks.21.ln1.hook_normalized', 'blocks.21.attn.hook_q', 'blocks.21.attn.hook_k', 'blocks.21.attn.hook_v', 'blocks.21.attn.hook_attn_scores', 'blocks.21.attn.hook_pattern', 'blocks.21.attn.hook_z', 'blocks.21.hook_attn_out', 'blocks.21.hook_resid_mid', 'blocks.21.hook_mlp_in', 'blocks.21.ln2.hook_scale', 'blocks.21.ln2.hook_normalized', 'blocks.21.mlp.hook_pre', 'blocks.21.mlp.hook_post', 'blocks.21.hook_mlp_out', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_pre', 'blocks.22.ln1.hook_scale', 'blocks.22.ln1.hook_normalized', 'blocks.22.attn.hook_q', 'blocks.22.attn.hook_k', 'blocks.22.attn.hook_v', 'blocks.22.attn.hook_attn_scores', 'blocks.22.attn.hook_pattern', 'blocks.22.attn.hook_z', 'blocks.22.hook_attn_out', 'blocks.22.hook_resid_mid', 'blocks.22.hook_mlp_in', 'blocks.22.ln2.hook_scale', 'blocks.22.ln2.hook_normalized', 'blocks.22.mlp.hook_pre', 'blocks.22.mlp.hook_post', 'blocks.22.hook_mlp_out', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_pre', 'blocks.23.ln1.hook_scale', 'blocks.23.ln1.hook_normalized', 'blocks.23.attn.hook_q', 'blocks.23.attn.hook_k', 'blocks.23.attn.hook_v', 'blocks.23.attn.hook_attn_scores', 'blocks.23.attn.hook_pattern', 'blocks.23.attn.hook_z', 'blocks.23.hook_attn_out', 'blocks.23.hook_resid_mid', 'blocks.23.hook_mlp_in', 'blocks.23.ln2.hook_scale', 'blocks.23.ln2.hook_normalized', 'blocks.23.mlp.hook_pre', 'blocks.23.mlp.hook_post', 'blocks.23.hook_mlp_out', 'blocks.23.hook_resid_post', 'blocks.24.hook_resid_pre', 'blocks.24.ln1.hook_scale', 'blocks.24.ln1.hook_normalized', 'blocks.24.attn.hook_q', 'blocks.24.attn.hook_k', 'blocks.24.attn.hook_v', 'blocks.24.attn.hook_attn_scores', 'blocks.24.attn.hook_pattern', 'blocks.24.attn.hook_z', 'blocks.24.hook_attn_out', 'blocks.24.hook_resid_mid', 'blocks.24.hook_mlp_in', 'blocks.24.ln2.hook_scale', 'blocks.24.ln2.hook_normalized', 'blocks.24.mlp.hook_pre', 'blocks.24.mlp.hook_post', 'blocks.24.hook_mlp_out', 'blocks.24.hook_resid_post', 'blocks.25.hook_resid_pre', 'blocks.25.ln1.hook_scale', 'blocks.25.ln1.hook_normalized', 'blocks.25.attn.hook_q', 'blocks.25.attn.hook_k', 'blocks.25.attn.hook_v', 'blocks.25.attn.hook_attn_scores', 'blocks.25.attn.hook_pattern', 'blocks.25.attn.hook_z', 'blocks.25.hook_attn_out', 'blocks.25.hook_resid_mid', 'blocks.25.hook_mlp_in', 'blocks.25.ln2.hook_scale', 'blocks.25.ln2.hook_normalized', 'blocks.25.mlp.hook_pre', 'blocks.25.mlp.hook_post', 'blocks.25.hook_mlp_out', 'blocks.25.hook_resid_post', 'blocks.26.hook_resid_pre', 'blocks.26.ln1.hook_scale', 'blocks.26.ln1.hook_normalized', 'blocks.26.attn.hook_q', 'blocks.26.attn.hook_k', 'blocks.26.attn.hook_v', 'blocks.26.attn.hook_attn_scores', 'blocks.26.attn.hook_pattern', 'blocks.26.attn.hook_z', 'blocks.26.hook_attn_out', 'blocks.26.hook_resid_mid', 'blocks.26.hook_mlp_in', 'blocks.26.ln2.hook_scale', 'blocks.26.ln2.hook_normalized', 'blocks.26.mlp.hook_pre', 'blocks.26.mlp.hook_post', 'blocks.26.hook_mlp_out', 'blocks.26.hook_resid_post', 'blocks.27.hook_resid_pre', 'blocks.27.ln1.hook_scale', 'blocks.27.ln1.hook_normalized', 'blocks.27.attn.hook_q', 'blocks.27.attn.hook_k', 'blocks.27.attn.hook_v', 'blocks.27.attn.hook_attn_scores', 'blocks.27.attn.hook_pattern', 'blocks.27.attn.hook_z', 'blocks.27.hook_attn_out', 'blocks.27.hook_resid_mid', 'blocks.27.hook_mlp_in', 'blocks.27.ln2.hook_scale', 'blocks.27.ln2.hook_normalized', 'blocks.27.mlp.hook_pre', 'blocks.27.mlp.hook_post', 'blocks.27.hook_mlp_out', 'blocks.27.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    tokens,\n",
    "    remove_batch_dim=True\n",
    ")\n",
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824cd4a-241e-4e92-874c-602db1f6e197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilya] *",
   "language": "python",
   "name": "conda-env-ilya-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
