{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc783e1-749d-4392-8c14-d0591b60745d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup (just run all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a1cb5-c424-4360-bf92-0a82a960c0e8",
   "metadata": {},
   "source": [
    "## Auxilliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c24af3-6c6c-48c2-8b4e-2562a67a868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file as safetensors_load_file\n",
    "\n",
    "from sae_lens.sae import SAE, SAEConfig\n",
    "\n",
    "\n",
    "def _dtype_to_cfg_str(dtype: torch.dtype | str | None) -> str:\n",
    "    if dtype is None:\n",
    "        return \"torch.float32\"\n",
    "    if isinstance(dtype, torch.dtype):\n",
    "        return str(dtype)\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def _device_to_cfg_str(device: torch.device | str | None) -> str:\n",
    "    if device is None:\n",
    "        return \"cpu\"\n",
    "    if isinstance(device, torch.device):\n",
    "        return str(device)\n",
    "    return device\n",
    "\n",
    "\n",
    "def _build_sae_cfg_from_training(\n",
    "    *,\n",
    "    model_name: str,\n",
    "    hook_layer: int,\n",
    "    d_in: int,\n",
    "    d_sae: int,\n",
    "    context_size: int = 128,\n",
    "    dataset_path: str = \"ashaba1in/small_openwebtext\",\n",
    "    hook_name: str = \"blocks.{layer}.hook_{target}\",  # hook_mlp_in  hook_resid_pre,\n",
    "    target: str = \"resid_pre\",  # mid_pre, resid_pre,\n",
    "    device: torch.device | str | None = \"cuda\",\n",
    "    dtype: torch.dtype | str | None = \"torch.float32\",\n",
    "    hook_head_index: Optional[int] = None,\n",
    ") -> SAEConfig:\n",
    "    target = \"mlp_in\" if target == \"mid_pre\" else \"resid_pre\"\n",
    "    \n",
    "    cfg_dict = {\n",
    "        \"architecture\": \"standard\",\n",
    "        \"d_in\": int(d_in),\n",
    "        \"d_sae\": int(d_sae),\n",
    "        \"activation_fn_str\": \"relu\",\n",
    "        \"activation_fn_kwargs\": {},\n",
    "        \"apply_b_dec_to_input\": True,\n",
    "        \"finetuning_scaling_factor\": False,\n",
    "        \"context_size\": int(context_size),\n",
    "        \"model_name\": model_name,\n",
    "        \"hook_name\": hook_name.format(layer=hook_layer, target=target),\n",
    "        \"hook_layer\": int(hook_layer),\n",
    "        \"hook_head_index\": hook_head_index,\n",
    "        \"prepend_bos\": False,\n",
    "        \"dataset_path\": dataset_path,\n",
    "        \"dataset_trust_remote_code\": False,\n",
    "        \"normalize_activations\": \"none\",\n",
    "        \"dtype\": _dtype_to_cfg_str(dtype),\n",
    "        \"device\": _device_to_cfg_str(device),\n",
    "        \"sae_lens_training_version\": None,\n",
    "        \"neuronpedia_id\": None,\n",
    "        \"model_from_pretrained_kwargs\": {},\n",
    "        \"seqpos_slice\": (None,),\n",
    "    }\n",
    "    return SAEConfig.from_dict(cfg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499c567b-5888-4b54-86ba-ed30894df44b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "from safetensors.torch import load_file, save_file\n",
    "import os\n",
    "\n",
    "def get_custom_hf_model(model_name: str, kwargs: Dict[str, Any] = {}) -> HookedTransformer:\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        **kwargs\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "    )\n",
    "    \n",
    "    hf_config = hf_model.config\n",
    "    \n",
    "    # Создаем конфигурацию для TransformerLens\n",
    "    # Ограничиваем размер контекста для экономии памяти\n",
    "    max_ctx = min(hf_config.max_position_embeddings, 2048)\n",
    "    \n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=hf_config.num_hidden_layers,\n",
    "        d_model=hf_config.hidden_size,\n",
    "        d_head=hf_config.hidden_size // hf_config.num_attention_heads,\n",
    "        n_heads=hf_config.num_attention_heads,\n",
    "        d_mlp=hf_config.intermediate_size,\n",
    "        d_vocab=hf_config.vocab_size,\n",
    "        n_ctx=max_ctx,  # Ограничиваем размер контекста\n",
    "        act_fn=hf_config.hidden_act,  # Llama использует SiLU\n",
    "        model_name=model_name,\n",
    "        normalization_type=\"RMS\",  # Llama использует RMSNorm\n",
    "        device=\"cpu\", \n",
    "        use_hook_mlp_in=True,\n",
    "    )\n",
    "    \n",
    "    model = HookedTransformer(cfg)\n",
    "    \n",
    "    model.load_state_dict(hf_model.state_dict(), strict=False)\n",
    "    model.set_tokenizer(tokenizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_sae_and_logs(\n",
    "    src_file = \"Llama-2.3-3B-Instruct-special_blocks.-4.hook_resid_pre_18432.safetensors\",\n",
    "    log_file = None,\n",
    "    layer_range = None,\n",
    "):\n",
    "\n",
    "    # ==== 1. качаем исходный файл ====\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=SRC_REPO,\n",
    "        filename=src_file,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    log_path = hf_hub_download(\n",
    "        repo_id=SRC_REPO,\n",
    "        filename=src_file.replace(\".safetensors\", \"_log_feature_sparsity.pt\") if log_file is None else log_file,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "\n",
    "    weights = load_file(local_path)\n",
    "    logs = torch.load(log_path, weights_only=True)\n",
    "\n",
    "    if layer_range is None:\n",
    "        start, _ = extract_layer_range(src_file)\n",
    "    else:\n",
    "        start, _ = layer_range\n",
    "\n",
    "    sae_by_layer = {}\n",
    "    for k, v in weights.items():\n",
    "        layer_id, weight_name = k.split(\".\")\n",
    "        layer_id = start + int(layer_id)\n",
    "        if layer_id not in sae_by_layer:\n",
    "            sae_by_layer[layer_id] = {weight_name: v}\n",
    "        sae_by_layer[layer_id][weight_name] = v\n",
    "\n",
    "    return sae_by_layer, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e46255f-30ae-478f-8c82-1a2299c8ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sae-lens transformer-lens sae-dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa24b-03ff-4418-9db6-fd40ef2cfb83",
   "metadata": {},
   "source": [
    "# SAE gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0562777b-3a89-45fb-bcad-4c273263b9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1658afe02d84c9a8f428572325ada62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sae_lens import HookedSAETransformer\n",
    "\n",
    "\n",
    "def load_hookedtrans_and_sae(\n",
    "    hf_model = \"ExplosionNuclear/Llama-2.3-3B-Instruct-special\", \n",
    "    layer = 1, \n",
    "    hook_target = \"mid_pre\"  # mid_pre  resid_pre\n",
    ") -> Tuple[HookedTransformer, SAE]:\n",
    "    hook_trans = get_custom_hf_model(hf_model)\n",
    "    hook_trans.to(\"cuda\")\n",
    "    \n",
    "    repo_id = \"Lucid-Layers-Inc/Llama-3.2-3B-Instruct-special-SAE\"\n",
    "    repo_id = repo_id.replace(\"special\", \"special-merged\") if \"special-merged\" in hf_model else repo_id\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=f\"Llama-2.3-3B-Instruct-special_layer-{layer}.hook_{hook_target}_18432.safetensors\",\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    \n",
    "    weights = load_file(local_path)\n",
    "    hidden_dim, d_in = weights[\"W_dec\"].shape\n",
    "    \n",
    "    sae_cfg = _build_sae_cfg_from_training(d_sae=hidden_dim, d_in=d_in, hook_layer=layer, model_name=hf_model, target=hook_target)\n",
    "    sae = SAE(sae_cfg)\n",
    "    sae.load_state_dict(weights, strict=True)\n",
    "    return hook_trans, sae\n",
    "\n",
    "model, sae = load_hookedtrans_and_sae(layer=10)\n",
    "\n",
    "# Если нужна поддержка HookedSAETransformer - вроде трюк сработал\n",
    "model.__class__ = HookedSAETransformer\n",
    "if not hasattr(model, \"acts_to_saes\"):\n",
    "    model.acts_to_saes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4966f-b1a1-4e27-ad50-497a1aa6d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://9dc51eb84c1723b73a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9dc51eb84c1723b73a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blocks.10.hook_mlp_in'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "def render_colored(tokens, strengths):\n",
    "    # нормируем в [0,1] для альфы цвета\n",
    "    s = torch.clamp(strengths, min=0)\n",
    "    s = (s / (s.max() + 1e-8)).tolist()\n",
    "    html = []\n",
    "    for tok, a in zip(tokens, s):\n",
    "        # оранжевый heatmap по активации\n",
    "        html.append(f'<span style=\"background-color: rgba(255,165,0,{a:.3f}); padding:2px; margin:1px; border-radius:3px\">{tok}</span>')\n",
    "    return \"<div style='font-family:monospace; line-height:2.0'>\" + \" \".join(html) + \"</div>\"\n",
    "\n",
    "@torch.inference_mode()\n",
    "def inspect_feature(prompt: str, feature_id: int):\n",
    "    # 1) токенизация\n",
    "    toks = model.to_tokens(prompt)\n",
    "    str_toks = model.to_str_tokens(prompt)\n",
    "\n",
    "    # 2) получаем активации узла для SAE и считаем фичи\n",
    "    # ВАЖНО: SAE не должен быть \"приклеен\" к модели во время кэширования.\n",
    "    out, cache = model.run_with_cache(\n",
    "        toks, return_type=\"logits\",\n",
    "        names_filter=[sae.cfg.hook_name],\n",
    "        remove_batch_dim=True\n",
    "    )\n",
    "    print(cache.keys())\n",
    "    x = cache[sae.cfg.hook_name]                      # [pos, d_in]\n",
    "    feats = sae.encode(x)                             # [pos, d_sae]\n",
    "    f = feats[:, feature_id].float().cpu()            # [pos]\n",
    "\n",
    "    # 3) топ токены словаря по направлению декодера (логит-влияние при активации=1)\n",
    "    # dir_vocab[v] ≈ (W_dec[f] · W_U[:, v])\n",
    "    W_U = model.W_U                                   # [d_model, d_vocab]\n",
    "    wdec_f = sae.W_dec[feature_id].to(W_U.dtype).to(W_U.device)  # [d_in]\n",
    "    dir_vocab = (wdec_f @ W_U).float().cpu()          # [d_vocab]\n",
    "    vals, idx = torch.topk(dir_vocab, 15)\n",
    "    toks_top = [model.to_string([i.item()]) for i in idx]\n",
    "\n",
    "    heatmap_html = render_colored(str_toks, f)\n",
    "    table = {t: float(v) for t, v in zip(toks_top, vals)}\n",
    "    stats = {\n",
    "        \"mean_act\": float(torch.relu(f).mean()),\n",
    "        \"max_act\": float(torch.relu(f).max()),\n",
    "        \"sparsity(frac>0)\": float((torch.relu(f) > 0).float().mean()),\n",
    "        \"d_sae\": int(sae.cfg.d_sae),\n",
    "        \"hook\": sae.cfg.hook_name,\n",
    "    }\n",
    "    return heatmap_html, table, stats\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=inspect_feature,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=4, label=\"Prompt\", value=\"The capital of France is Paris.\"),\n",
    "        gr.Slider(0, sae.cfg.d_sae-1, value=0, step=1, label=\"Feature ID\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.HTML(label=\"Token activations (highlighted)\"),\n",
    "        gr.Label(num_top_classes=15, label=\"Top vocab (decoder direction)\"),\n",
    "        gr.JSON(label=\"Stats\"),\n",
    "    ],\n",
    "    title=\"SAE Feature Explorer\",\n",
    "    description=\"Подсветка активаций признака и его влияния на логиты.\",\n",
    ")\n",
    "demo.launch(inline=True, share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb4708-2c41-4cb5-8b26-3cf6ea7f04e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a41ca-8c32-449f-b919-05f82e0593c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
